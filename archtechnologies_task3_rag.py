# -*- coding: utf-8 -*-
"""ArchTechnologies-Task3-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RO5oC0whRkHHO9V5gYUFzE4es5ZwSYrk
"""

!nvidia-smi

!pip show unsloth transformers accelerate faiss-gpu sentence-transformers langchain chromadb

!pip show unsloth faiss-gpu chromadb

!pip install -q unsloth faiss-cpu chromadb

!pip show unsloth faiss-cpu chromadb

!du -sh /usr/local/lib/python3.11/dist-packages/unsloth
!du -sh /usr/local/lib/python3.11/dist-packages/faiss*
!du -sh /usr/local/lib/python3.11/dist-packages/chromadb*

!nvidia-smi

pip show unsloth

pip show faiss-cpu

pip show chromadb

import unsloth, faiss, chromadb
print("All 3 import successfully ‚úÖ")

# ----------------- STEP 2: CREATE DOCUMENTS + EMBEDDINGS -----------------

from chromadb import Client
from chromadb.utils import embedding_functions

# Initialize Chroma client
chroma_client = Client()

# Create a collection (like a table in DB)
collection = chroma_client.create_collection(
    name="my_rag_collection",
    embedding_function=embedding_functions.DefaultEmbeddingFunction()
)

# Example documents (you can replace these with your own knowledge base)
documents = [
    "Artificial Intelligence is the simulation of human intelligence in machines.",
    "Machine Learning is a subset of AI that allows computers to learn from data.",
    "Deep Learning uses neural networks with many layers to model complex patterns.",
    "Natural Language Processing enables machines to understand human language."
]

# Add documents into Chroma collection
collection.add(
    documents=documents,
    ids=[f"doc_{i}" for i in range(len(documents))]
)

print("‚úÖ Documents added to ChromaDB successfully!")

# ----------------- STEP 3: QUERY THE DATABASE -----------------

# Example query
query = "What is deep learning?"

# Search in ChromaDB (top 2 most relevant documents)
results = collection.query(
    query_texts=[query],
    n_results=2
)

print("üîç Query:", query)
print("\nüìå Retrieved Documents:")
for doc in results["documents"][0]:
    print("-", doc)

# ----------------- STEP 4: LOAD UNSLOTH 4-BIT MODEL -----------------
import torch
from unsloth import FastLanguageModel

# A good starting choice. If this exact name 404s, try one of:
#   "unsloth/Llama-3.1-8B-Instruct-bnb-4bit"
#   "unsloth/Meta-Llama-3-8B-Instruct-bnb-4bit"
#   "unsloth/Llama-2-7b-bnb-4bit"
MODEL_NAME = "unsloth/Llama-3.1-8B-Instruct-bnb-4bit"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = MODEL_NAME,
    load_in_4bit = True,        # dynamic 4-bit quantization via bitsandbytes
    dtype = None,               # let Unsloth pick optimal dtype
    device_map = "auto",        # place layers across available GPUs/CPU if needed
)

# Optional: speed tricks Unsloth exposes
FastLanguageModel.for_inference(model)  # enables faster inference kernels

print("‚úÖ Loaded:", MODEL_NAME)
print("CUDA available:", torch.cuda.is_available())

from transformers import pipeline

# Wrap the model + tokenizer into a generation pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Test with a simple prompt
response = pipe(
    "Hello! Tell me something about artificial intelligence.",
    max_new_tokens=200,
    temperature=0.7,
    top_p=0.9
)

print(response[0]["generated_text"])

from datasets import load_dataset

# Load Alpaca dataset
dataset = load_dataset("yahma/alpaca-cleaned", split="train")

# Check a random sample
print(dataset[0])

from sentence_transformers import SentenceTransformer
import chromadb

# 1Ô∏è‚É£ Initialize Chroma vector store
client = chromadb.Client()

# ‚úÖ Use get_or_create_collection to avoid duplicate errors
collection = client.get_or_create_collection(name="alpaca_collection")

# 2Ô∏è‚É£ Initialize embedding model
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# 3Ô∏è‚É£ Split dataset and add to Chroma
for idx, item in enumerate(dataset[:100]):  # limit to 100 for testing
    text = item['instruction'] + " " + item['input'] + " " + item['output']
    vector = embed_model.encode(text).tolist()
    collection.add(
        ids=[str(idx)],
        metadatas=[{"text": text}],
        embeddings=[vector]
    )

print("‚úÖ Dataset embedded and added to Chroma (safe mode)!")

print(dataset[0])

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import chromadb

# 1Ô∏è‚É£ Load dataset
dataset = load_dataset("yahma/alpaca-cleaned", split="train")

# 2Ô∏è‚É£ Init ChromaDB + embeddings
client = chromadb.Client()
collection = client.create_collection("alpaca_rag")
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# 3Ô∏è‚É£ Loop properly (each row is dict)
for idx in range(100):  # just 100 for testing
    item = dataset[idx]
    text = item["instruction"] + " " + item["input"] + " " + item["output"]
    vector = embed_model.encode(text).tolist()

    collection.add(
        ids=[str(idx)],
        embeddings=[vector],
        documents=[text]
    )

print("‚úÖ Data inserted into ChromaDB")

# Query Chroma for similar results
query_text = "How can I stay healthy?"
query_vector = embed_model.encode(query_text).tolist()

results = collection.query(
    query_embeddings=[query_vector],
    n_results=3  # how many similar documents you want
)

print("Query:", query_text)
print("Results:")
for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
    print("\n---")
    print("Document:", doc)
    print("Metadata:", meta)

def rag_query(query, top_k=3):
    # Step 1: Search in vector DB
    results = collection.query(query_texts=[query], n_results=top_k)

    # Step 2: Extract retrieved docs (not metadata['text'])
    context = " ".join(results["documents"][0])

    # Step 3: Build prompt for local Llama
    prompt = f"""
    You are a helpful assistant. Use the context to answer.

    Context:
    {context}

    Question: {query}
    Answer:
    """

    # Step 4: Run local Llama model
    output = pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)

    return output[0]["generated_text"]

print(rag_query("What is AI?"))
print(rag_query("How to prevent diabetes?"))
print(rag_query("Explain machine learning in simple words"))